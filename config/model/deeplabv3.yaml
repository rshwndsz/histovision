# DeepLabV3
# Paper: https://arxiv.org/abs/1706.05587
model:
  name: deeplabv3
  class: segmentation_models_pytorch.DeepLabV3
  params:
    # Name of classification model (without last dense layers) used as feature extractor to build segmentation model
    encoder_name: densenet121
    # Number of stages used in decoder, larger depth - more features are generated
    # Ex: for depth=3 encoder will generate list of features with following spatial shapes
    # [(H,W), (H/2, W/2), (H/4, W/4), (H/8, W/8)], so in general the deepest feature will have
    # spatial resolution (H/(2^depth), W/(2^depth)]
    encoder_depth: 5
    # One of [NULL => None (random initialization), imagenet => (pre-training on ImageNet)]
    encoder_weights: imagenet
    # Number of convolution filters in ASPP module (default 256).
    decoder_channels: 256
    # spatial dropout rate between 0 and 1
    psp_dropout: 0.2
    # Number of input channels for model, default is 3
    in_channels: 3
    # Number of classes for output
    # output shape => [N C H W]
    classes: 2
    # activation function used in `.predict(x)` method for inference
    # One of [sigmoid, softmax2d, callable, None]
    activation: NULL
    # optional, final upsampling factor
    # (default is 8 to preserve input -> output spatial shape identity)
    upsampling: 8
    # if specified model will have additional classification auxiliary output
    # build on top of encoder, supported params:
    #    - classes (int): number of classes
    #    - pooling (str): one of 'max', 'avg'. Default is 'avg'
    #    - dropout (float): dropout factor in [0, 1)
    #    - activation (str): activation function to apply "sigmoid"/"softmax" (could be None to return logits)
    aux_params: NULL
